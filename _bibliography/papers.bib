---
---

@string{aps = {American Physical Society,}}

@inproceedings{yan2025don,
  abbr={ICLR},
  title={Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models},
  author={Yan, Shaotian and Shen*, Chen and Wang, Wenxiao and Xie, Liang and Liu, Junjie and Ye, Jieping},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  abstract={Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning capabilities of large language models (LLMs), functioning as a whole to guide these models in generating reasoning steps toward final answers. However, we observe that isolated segments, words, or tokens within CoT demonstrations can unexpectedly disrupt the generation process of LLMs. The model may overly concentrate on certain local information present in the demonstration, introducing irrelevant noise into the reasoning process and potentially leading to incorrect answers. In this paper, we investigate the underlying mechanism of CoT through dynamically tracing and manipulating the inner workings of LLMs at each output step, which demonstrates that tokens exhibiting specific attention characteristics are more likely to induce the model to take things out of context; these tokens directly attend to the hidden states tied with prediction, without substantial integration of non-local information. Building upon these insights, we propose a Few-shot Attention Intervention method (FAI) that dynamically analyzes the attention patterns of demonstrations to accurately identify these tokens and subsequently make targeted adjustments to the attention weights to effectively suppress their distracting effect on LLMs. Comprehensive experiments across multiple benchmarks demonstrate consistent improvements over baseline methods, with a remarkable 5.91% improvement on the AQuA dataset, further highlighting the effectiveness of FAI.},
  arxiv={2503.11154},
  selected={true}
}

@inproceedings{fanimproving,
  title={Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach},
  author={Fan, Sinan and Xie, Liang and Shen*†, Chen and Teng, Ge and Yuan, Xiaosong and Zhang, Xiaofeng and Huang, Chenxi and Wang, Wenxiao and He, Xiaofei and Ye, Jieping},
  booktitle={The Thirteenth International Conference on Learning Representations},
  abbr={ICLR},
  year={2025},
  arxiv={2503.13208},
  poster={6735_poster.pdf},
  selected={true}
}

@inproceedings{liu-etal-2025-concise,
    abbr={NAACL},
    title={Concise and Organized Perception Facilitates Reasoning in Large Language Models},
    author={Liu, Junjie  and
      Yan, Shaotian  and
      Shen*, Chen  and
      Xiao, Zhengdong  and
      Xie, Liang  and
      Wang, Wenxiao  and
      Ye, Jieping},
    booktitle={Findings of the Association for Computational Linguistics: NAACL 2025},
    year={2025},
    html={https://aclanthology.org/2025.findings-naacl.193/},
    arxiv={2310.03309},
    selected={true}
}

@inproceedings{zhang-etal-2025-redundancy,
    abbr={NAACL},
    title={From Redundancy to Relevance: Information Flow in {LVLM}s Across Reasoning Tasks},
    author={Zhang, Xiaofeng  and
      Quan, Yihao  and
      Shen†, Chen  and
      Yuan, Xiaosong  and
      Yan, Shaotian  and
      Xie, Liang  and
      Wang, Wenxiao  and
      Gu, Chaochen  and
      Tang, Hao  and
      Ye, Jieping},
    booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
    year={2025},
    html={https://aclanthology.org/2025.naacl-long.115/},
    arxiv={2406.06579},
    selected={true}
}

@inproceedings{NEURIPS2024_e304e04a,
 abbr={NeurIPS},
 author={Yuan, Xiaosong and Shen†, Chen and Yan, Shaotian and Zhang, Xiaofeng and Xie, Liang and Wang, Wenxiao and Guan, Renchu and Wang, Ying and Ye, Jieping},
 booktitle={Advances in Neural Information Processing Systems},
 title={Instance-adaptive Zero-shot Chain-of-Thought Prompting},
 html={https://proceedings.neurips.cc/paper_files/paper/2024/file/e304e04a6f455dd82f8a85a0a3679493-Paper-Conference.pdf},
 year={2024},
 arxiv={2409.20441},
 selected={true}
}
